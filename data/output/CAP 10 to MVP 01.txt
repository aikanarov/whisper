would like to see and then that way we can start to estimate that work and work with you to get things in place so that when we're ready to transition we can transition over and start to you know test it and work with it in a U.S. environment. So that's really kind of the intent of the call is just to get us back on track and moving forward. Okay, that sounds really great. So where do we want to start? Do we want to revisit where we left off with the use case and what we developed? I think you have some new folks with us? Yes, probably Chris and Melissa. This might be the first time there. Actually Chris, I think, okay perfect. I wasn't sure if you had been pulled in a little earlier. So yeah, Autumn, why don't we get everyone up to speed? Yeah, let's do that. So Juan or Jose, can we walk the team through what we've created for them in the past or so far and where we ended? Yes, let me find the presentation we have. Yeah, I will use that. Give me a second, please. Okay. Okay, here I have the presentation. Give me a second. Okay. Hello. Well, the idea of this use case was, Kevin, if you want, you can run a little, but the idea was able to review the internal information of an assessment being made to an institution and use generative AI to discover and extract all the risks that it can be found on that document. Also, going through the control catalog to extract all the controls in place and then being able to do a mapping where we're able to correlate if a risk is covered by one of those internal controls. So we create a process where we upload the client information. We upload the BSA AML control documents. Then we have a process that reads and parses all of those documents and information. We extract the risks and also the controls using large language models. We do a machine learning process to remove duplicates because sometimes on these risk documents, you can have the same risks being stated a couple of times in the same document. So we try to minimize that information. And then we create this mapping to create the final Excel output. In general terms, this is a lightweight POC we create using a platform called Streamlit where you can select the process. One of the first requirements we have there is that it was that sometimes the users may need to modify the prompt, use it for the process, or add additional instructions because maybe they are using, extracting or working with some specifics or they want the model to focus on something more than in other use cases. So we enable that process. So the idea is they upload the client information, the risk assessment, they upload the control file. And the first part is, and then we add a threshold to be able to see how tight the duplicates are removed. So we review, we can see all the risks being extracted from the documents, all the controls we extracted from the documents. And in this case, they asked us to be able to provide the possibility to download this middle step before doing the mappings. So the consultant is able to, I don't know, make any adjustments in terms maybe to remove something or other. One side thing that's been done here is that the risks are being categorized with a list of possible categories that was provided. So that way, you don't have only the risk, but to what category of risk this is, how do you say, it's related to. And then we upload again, the both documents, the ones that were downloaded. And we started like the mapping process to see these correlations. And when this is done, you're able to download an Excel file where you can have all that information from. You can see like a one-to-many and many-to-one relationship. So which controls are related to which controls, which risks are related with which controls, and which controls are related with each of those risks in this workbook. And that's like the scope of the first CAP10 process. I don't know, Kevin, you want to add up something that maybe I missed? Generally speaking, yeah, that was the scope of the application where we're incorporating a bit of additional features like the link between the risk and the page where the risk is being retrieved, and also with the controls. So the reviewer could have a better understanding of where the large language model is retrieving the information. Yeah, generally speaking, it's that you showed. So that's the process. I don't know if you have any questions about the process. No, I have the business. Yeah. Oh, go ahead, Autumn. No, I'm saying you have the business. Yeah, definitely have questions or anything that you'd like. Yeah, thank you, Jose, for taking us through this. So I believe the reason why we had initially went down this path was to see how we could automate some of our risk assessment and mapping processes, not only for the financial crime side of the house, but the regulatory compliance side of the house, particularly when we're supporting clients with internal audit or helping with the overall risk assessment for those programs. And so this is supporting in an automated way, which, you know, today when we do this work, it does take a lot of extra time for us and some additional thought. So I think, Chris, that was sort of the initial idea on how do we automate that process and take some time out of it. So this isn't just limited to like model validating? That's right. Yes, that's correct. It was intended to be more broad across the across the technical capabilities. Okay. That's why there's some flexibility in the process. That's why there's some flexibility in the regulations in which it points to as the starting point. Right. So it's been trained across AML and consumer compliance regs or is it? I think we're just starting right now on the process. So we haven't done anything, correct? Only to specify something that we're using large language models and these models are not trained. So we manage like the information we add in the context. So if you want and that's one of the nice things, because if you want to add additional regs, well, you only need to process them. And that could be added with no need to modify like the underlying process. Okay. So you just kind of add it, add what you want to the library that it's serving again? Yeah, exactly. Okay.
And so where exactly are we? I guess what I'm confused about is where are we with this prototype? Like, are we able to start putting this in the hands of the team? Do you still have more development to go? I guess I'm very unclear about where we are in the process. Yep. So what you see today is what you have and what we've developed, right? And so the question for today is, is that what you want to put in people's hands? Or do we want to expand that? Because again, when we prototype, we do it within a very specific time to test it. But that doesn't always mean that's exactly what you want to bring into production. And so one, we'd want to know, do you want to add anything in addition to it, expand it in any way? And then based on that, what we would start to do now that we, again, are getting closer to being able to bring what they've created to a US environment. Once we get it into a US environment, we would ask your team to test it using your client data projects that you have that we could test and validate what has been created for you. And as we test it and get it more refined and you feel comfortable at certain point that it does what it's supposed to be doing, then we would release it into production and you would be able to start to use it more widely. Okay. So what, so I guess I'm, I'm sorry. So what we were just talking, like what we just walked through is that's every, that's the only thing we have? That's what was created as part of your POC. Yep. For, for your use case. Oh, okay. So I think that there's quite a bit more work that needs to happen before it can be put into people's hands. Yep. So we wanted to find what that is, Cynthia, like what, what would you need? What, what, what is there that's missing so that we can start to work through what that would look like in the timeline to get that developed in front of you? Well, I think first and foremost, we don't have capacity to even start thinking about this until after the new calendar year as a team. So I was mistaking under the impression we were much further along than what was just showed to us. So that was obviously myself being completely misinformed. Where did, so Cynthia, were you in some of the demos before when we were doing the use case? I don't know how many demos I've been in. Maybe the initial, the initial kind of discussion where we gathered input. Yes. I provided input. I believe we provided some follow-up documents around what this process looks like today in an Excel manner. I think that's kind of where some of this was built or as the starting point for what was shown by, I think this was the first time I'm actually seeing any of the dev work that's been done. Yeah. And we can look back. I'm not sure who we demoed it with. I don't know if Jose or Juan, if you remember who we did the final demo and read out with? Well, we probably did it with Nico. Okay. He's not here anymore. He's not here anymore. Yes. It was, it was with me. Yeah. Yeah. So that's where we've left it. Maybe could help our, the files on the SharePoint or final report. No, but this was like a, what you saw is what a was defined at the beginning as a proof of concept to validate because we were trying to validate if it was possible to do this process. I think it's normal that it's, it needs extra development to take it to a minimum viable product when we need to add all the other things that are needed for, I don't know, to automate that process. And that was the scope being provided by Nicolai at the moment and how we close like the initial six weeks of work on this, four or six, something like that. So what do you think is missing here? So you can think that this is like a minimum viable product, something like the minimum things that you need for this to be, I don't know, to prove, to provide value in the day to day work. Well, first off, how would anybody actually be able to use this in a test environment based on what you showed? I just saw a couple of screenshots and a PowerPoint presentation. So I'm not, I'm not visualizing how we would put this in hands of our consultants to actually use it to complete work. Yeah, for that, the idea, and that's part of this idea, we create a software that operates, that it uses a, how do you say, let's say a web page. But the idea is that and what Adam and all the CAP team has been working is find a path with a internal, with IT, where this can be deployed on the internal RSM net, how do you say, infrastructure. So if you have a consultant, they should be able to log in to an URL on the, on RSM net with maybe an authentication with their normal credentials. And they will have like this interface to upload their documents and have the process and do the work. That makes sense? Yes, but that's not built, right? No, that's part of deploying it to the U.S. environment, Cynthia. So that's where we want the model to work. We want it to get the right results for you. And then we will package it up and get it into a U.S. environment so that your team can access it and then test it, right? And start to work with it. So that would be part of moving it over. But the important part before we fully move it over is making sure that it covers your use case, right? And it works properly. And then finding again, as Jose said, putting it out on the web or making it accessible is kind of the deployment piece of it. Okay. I'm probably just completely missing something. I don't know. I mean, Chris and Melissa, based on what we saw, are you able to roll this out to the team to start testing it? I mean, I guess it's my first time seeing it. So I need to, I probably need to do a little homework on what the actual use case presented was. I mean, conceptually, it sounds like it's doing, you know, in terms of a coverage assessment, it is doing what we outlined. I guess I would need to see the output a little more broadly. The input and the output, I agree. Like, yes, it feels like directionally, it's the right thing. But without having anything tangible that people can actually use, it's hard to say if this is what we would expect. Let me grab some ideas here. Maybe first, we could have Kevin and Juan make a more deep presentation to maybe Chris or anybody.
else in the team so they can see the input documents and how this operates in a live environment. Then, as Adam says, we create an application that can be run, but it can't run using customer information or in RSMnet environment because of IT and security policies. That's why we are prototyping this outside RSMnet. If you see that the functionality, the process, it's, how do you say, aligned and in theory it could help improve the work, then we can work into deploying it into RSM infrastructure. That can be a very, maybe correct me, Adam, but like the CAP44 with an executable for the UAT testing or directly to a production ready solution. But this is, that is part of the process. So the idea is that you first validate the concept, yes, GNI is able to solve or help in this process and save hours if this increase. Then if that is okay, let's deploy and start a testing. And for that, we will provide the, with RSM US team, we will provide the application, the URL, whatever, so the team can make the test. With the feedback, we create all the, I don't know, with just the process, we improve it. And with, I don't know, the tech, then we can deploy it for production. I don't know if that process makes sense. And please correct me, Adam, if I missed something. No, I don't think you're missing things. I think Cynthia for us is like giving you guys some more time with asking questions and actually understanding what it does would probably be helpful. And again, I don't know, is that like a document we could put together to say, here's what inputs we have, here's what the model can do, and what is covered. Do you want to start there first and we can just outline that for you, or would it be more helpful to sit and actually walk through it in a lot more detail together and like take one of your, take the process itself and let's talk through it with the model? What would be more helpful? We probably need to look at the model in more detail, like do a walkthrough, potentially the walkthrough that you had with Nico and maybe a little bit more involved. Yeah. Because I'm sure the walkthrough that you had with Nico involved just looking at the code. Yeah, just like step by step process of the documents, this is what it's doing, here's what it's, yeah, the output, blah, blah, blah, based off of the requirements. We could, yeah, be a little bit more detailed on that. Yeah. Like here's the input. This is where you would upload this. This is where you go to get your, you know, to point to the regulatory compliance requirement. Like this is how you know that you're kind of pointing to the right requirements, right? Because your starting point here is a reg. And then we're essentially pulling in, I believe, you know, a risk library, right? That then gets more focused, narrowed down based by the focus of what regulatory requirements are in scope. And then, sorry, just a quick question. So this application, so I guess like, where, so I guess, does it start from extracting the risk of the client, or is it both the risk of the client and the regulation risks? So I guess like, what documents are we able to input into this application for them to extract? I can show the documents that I used to test the application. Let me, give me a second. Okay. So this was the document that Nicolai sent us regarding the risk. It's a long document, and the basic idea was to retrieve as many documents as possible. It has some missing information regarding security, but the model is able to first read like all these texts for saying something, and then understanding through prompting, which relate to a control and which thing relate to a risk. So here's the risk document, and here's the control document. Also provided by Nicolai. So the process is the same for each document, but one is focusing on risk retrieval, and the other one is focusing on control retrieval. Got it, okay. So actually I can show you like the documents. Like I was saying before, this is something that we are working on currently. Like for example, you have here this risk, which is a complicated structure, and so on. You can actually click here, and it will send you to the page of the document in the same Excel file. We kind of understand that this is very helpful for the reviewer, like on really understanding where the document is retrieving the information. So in that sense, we find it very helpful. So in the first part of the video that you saw, that's actually what the model is doing. It's taking, for example, this document, and it's retrieving all of these potential risks. So the idea is like the reviewer can check this, and for example, here you can eliminate that or something that you find it is not very correct. You can modify it, and you can also input some risks that you find that the model is missing. And then with that file done and ready.